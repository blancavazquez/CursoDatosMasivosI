{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3c_minhash.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IvxCNshmnMDh"
      ],
      "authorship_tag": "ABX9TyMVeAW8p0sohP4NFbm1yvc5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blancavazquez/CursoDatosMasivosI/blob/master/notebooks/3c_minhash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kloh2BluGySD"
      },
      "source": [
        "# Búsqueda de documentos con MinHash\n",
        "En esta libreta veremos cómo hacer búsqueda eficiente de documentos considerando la similitud de Jaccard.\n",
        "\n",
        "La similitud de Jaccard entre un par de conjuntos $(\\mathcal{C}^{(1)}, \\mathcal{C}^{(2)})$ está dada por\n",
        "\n",
        "$$\n",
        "J(\\mathcal{C}^{(1)}, \\mathcal{C}^{(2)}) = \\frac{\\mid \\mathcal{C}^{(1)} \\cap \\mathcal{C}^{(2)} \\mid}{\\mid \\mathcal{C}^{(1)}\\cup \\mathcal{C}^{(2)} \\mid} \\in [0,1]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOzaNGg6PA6_"
      },
      "source": [
        "from collections import Counter\n",
        "from math import floor, log\n",
        "import codecs \n",
        "import re \n",
        "\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix, lil_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "VOCMAX = 5000\n",
        "n_muestras_sint = 10000\n",
        "n_tablas_ng = 50\n",
        "\n",
        "# Para reproducibilidad\n",
        "np.random.seed(2021)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOhZkfFSzAQE"
      },
      "source": [
        "## Conjuntos de datos\n",
        "Preparamos dos conjuntos de datos para probar los algoritmos de búsqueda.\n",
        "\n",
        "### Datos sintéticos\n",
        "Este conjunto de datos está compuesto por 5 listas de elementos y un conjunto universo de 10 elementos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te6Uqrb6zFwS"
      },
      "source": [
        "sint_conj = [[0, 1, 4, 6, 8], [2, 3, 4, 7, 8], [1, 4, 6, 7], [0, 5, 6, 8], [0, 1, 3, 4, 7]]\n",
        "univ = {e for l in sint_conj for e in l}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AUc2a-ure1E"
      },
      "source": [
        "Calculamos la similitud de Jaccard de todos los pares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oab1l7qyfkUV",
        "outputId": "6f41413e-3e91-4f9c-97fe-a2eb9c199485"
      },
      "source": [
        "sims_jacc = np.identity(len(sint_conj))\n",
        "for i in range(0, len(sint_conj) - 1):\n",
        "  ci = set(sint_conj[i])\n",
        "  for j in range(i + 1, len(sint_conj)):\n",
        "    cj = set(sint_conj[j])\n",
        "    sims_jacc[i,j] = float(len(ci.intersection(cj)) / len(ci.union(cj)))\n",
        "    sims_jacc[j,i] = sims_jacc[i,j]\n",
        "\n",
        "print(sims_jacc)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.25       0.5        0.5        0.42857143]\n",
            " [0.25       1.         0.28571429 0.125      0.42857143]\n",
            " [0.5        0.28571429 1.         0.14285714 0.5       ]\n",
            " [0.5        0.125      0.14285714 1.         0.125     ]\n",
            " [0.42857143 0.42857143 0.5        0.125      1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lfZa7s6sE8U"
      },
      "source": [
        "Definimos una función que calcula la propiedad de colisión de todos los pares en este conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIYV78vTsaTk"
      },
      "source": [
        "def p_colision(db, tablas, n_tablas, n_ej):\n",
        "  for i in range(n_tablas):\n",
        "    for j,l in enumerate(db):\n",
        "      tablas[i].insertar(l, j)\n",
        "\n",
        "  colisiones = np.zeros((n_ej, n_ej))\n",
        "  for i in range(n_tablas):\n",
        "    for j,cj in enumerate(db):\n",
        "      for e in tablas[i].buscar(cj):\n",
        "        colisiones[j, e] += 1\n",
        "\n",
        "  return colisiones / n_tablas"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_56-act4ikel"
      },
      "source": [
        "### 20 Newsgroups\n",
        "Vamos a usar el conjunto de documentos de _20 Newsgropus_, el cual descargamos usando scikit-learn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrIWmtSvLthy"
      },
      "source": [
        "db = fetch_20newsgroups(remove=('headers','footers','quotes'))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drmTbXrFLdoL"
      },
      "source": [
        "Importamos la biblioteca NLTK y definimos nuestro analizador léxico y lematizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9HE8AS41yYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70aafeaf-780b-49b2-e881-6823d2b4e225"
      },
      "source": [
        "import nltk\n",
        "nltk.download(['punkt','averaged_perceptron_tagger','wordnet'])\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus.reader.wordnet import NOUN, VERB, ADV, ADJ\n",
        "\n",
        "morphy_tag = {\n",
        "    'JJ' : ADJ,\n",
        "    'JJR' : ADJ,\n",
        "    'JJS' : ADJ,\n",
        "    'VB' : VERB,\n",
        "    'VBD' : VERB,\n",
        "    'VBG' : VERB,\n",
        "    'VBN' : VERB,\n",
        "    'VBP' : VERB,\n",
        "    'VBZ' : VERB,\n",
        "    'RB' : ADV,\n",
        "    'RBR' : ADV,\n",
        "    'RBS' : ADV\n",
        "}\n",
        "\n",
        "def doc_a_tokens(doc):\n",
        "  tagged = pos_tag(word_tokenize(doc.lower()))\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = []\n",
        "  for p,t in tagged:\n",
        "    tokens.append(lemmatizer.lemmatize(p, pos=morphy_tag.get(t, NOUN)))\n",
        "\n",
        "  return tokens"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBHJ4RGvLzZB"
      },
      "source": [
        "Convertimos el conjunto preprocesado a una lista de cadenas, una por documento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv7kFB151Kpi"
      },
      "source": [
        "corpus = []\n",
        "for d in db.data:\n",
        "  d = d.replace('\\n',' ').replace('\\r',' ').replace('\\t',' ')\n",
        "  d = ' '.join([''.join([c.lower() for c in p if c.isalnum()]) for p in d.split()])\n",
        "  tokens = doc_a_tokens(d)\n",
        "  corpus.append(' '.join(tokens))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ado-nD45jFNP"
      },
      "source": [
        "Dividimos nuestro conjunto en 2 subconjuntos: los documentos de la base que se buscarán y los documentos de consulta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cduiS-ukgm7m"
      },
      "source": [
        "perm = np.random.permutation(len(corpus)).astype(int)\n",
        "n_ej_base = int(floor(len(corpus) * 0.95))\n",
        "\n",
        "base = [corpus[i] for i in perm[:n_ej_base]]\n",
        "consultas = [corpus[i] for i in perm[n_ej_base:]]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos y cargamos la lista de _stopwords_ para inglés (archivo con una palabra por línea)"
      ],
      "metadata": {
        "id": "ZbvR3-DPyElj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -qO- -O stopwords_english.txt \\\n",
        "         https://raw.githubusercontent.com/pan-webis-de/authorid/master/data/stopwords_english.txt\n",
        "\n",
        "stopwords = []\n",
        "for line in codecs.open('stopwords_english.txt', encoding = \"utf-8\"):\n",
        "  stopwords.append(line.rstrip())"
      ],
      "metadata": {
        "id": "EywC1QNIc9If"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procesamos cada palabra del corpus completo para generar y ordenar el vocabulario"
      ],
      "metadata": {
        "id": "Z46cJRKQyk2r"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3xcTfR7y8wE"
      },
      "source": [
        "# Divide la cadena en palabras\n",
        "term_re = re.compile(\"\\w+\", re.UNICODE)\n",
        "\n",
        "# Contamos las ocurrencias de cada palabra\n",
        "corpus_freq = Counter()\n",
        "doc_freq = Counter()\n",
        "for d in base:\n",
        "  # Eliminamos números de la cadena (documento) a procesar \n",
        "  d = re.sub(r'\\d+', '', d)\n",
        "\n",
        "  # Dividimos la cadena en una lista de palabras\n",
        "  terms = [t for t in term_re.findall(d) if t not in stopwords and len(t) > 2]\n",
        "  \n",
        "  # Aumentamos el contador de cada instancia palabra en el documento\n",
        "  for t in terms:\n",
        "    corpus_freq[t] += 1\n",
        "  \n",
        "  # Aumentamos el contador de cada palabra distinta en el documento\n",
        "  for t in set(terms):\n",
        "    doc_freq[t] += 1\n",
        "\n",
        "# Generamos un diccionario con las VOCMAX palabras más frecuentes\n",
        "vocabulary = {entry[0]:(i, entry[1], doc_freq[entry[0]], log(len(corpus) / doc_freq[entry[0]])) \\\n",
        "              for i, entry in enumerate(corpus_freq.most_common()) \\\n",
        "              if i < VOCMAX}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos un diccionario para mapear índices a palabras"
      ],
      "metadata": {
        "id": "K4yw8fgsA5SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id_a_palabra = {v[0]: k for k,v in vocabulary.items()}"
      ],
      "metadata": {
        "id": "riRxAcyuBFAy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3JFW9TJL-ne"
      },
      "source": [
        "Generamos las bolsas de palabras de los documentos preprocesados"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cadenas_a_bolsas(cadenas, voc, descartar, tre):\n",
        "  bolsas = []\n",
        "  for c in cadenas:\n",
        "    c = re.sub(r'\\d+', '', c)\n",
        "    ids = Counter([voc[t][0] for t in tre.findall(c) \\\n",
        "                   if t in voc and t not in descartar])\n",
        "    bolsas.append([i for i in sorted(ids.items())])\n",
        "\n",
        "  return bolsas\n",
        "\n",
        "bolsas_base = cadenas_a_bolsas(base, vocabulary, stopwords, term_re)\n",
        "bolsas_consultas = cadenas_a_bolsas(consultas, vocabulary, stopwords, term_re)"
      ],
      "metadata": {
        "id": "xIzDYE8beEhP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def csr_a_ldb(csr):\n",
        "  ldb = [[] for _ in range(csr.shape[0])]\n",
        "  coo = csr.tocoo()    \n",
        "  for i,j,v in zip(coo.row, coo.col, coo.data):\n",
        "    ldb[i].append(j)\n",
        "\n",
        "  return ldb\n",
        "\n",
        "def ldb_a_csr(ldb, dim):\n",
        "  n_el = 0\n",
        "  for i,l in enumerate(ldb):\n",
        "    n_el += len(l)\n",
        "\n",
        "  vals = np.zeros(n_el, dtype=float)\n",
        "  rows = np.zeros(n_el, dtype=int)\n",
        "  cols = np.zeros(n_el, dtype=int)\n",
        "  j = 0\n",
        "  for i,l in enumerate(ldb):\n",
        "    for e in l:\n",
        "      vals[j] = e[1]\n",
        "      rows[j] = i\n",
        "      cols[j] = e[0]\n",
        "      j += 1\n",
        "  return csr_matrix((vals, (rows, cols)), shape=(len(ldb), dim))\n",
        "\n",
        "bolsas_base_csr = ldb_a_csr(bolsas_base, VOCMAX)\n",
        "bolsas_consultas_csr = ldb_a_csr(bolsas_consultas, VOCMAX)"
      ],
      "metadata": {
        "id": "6vSWFfbN8xpY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgDZWHzhru4b"
      },
      "source": [
        "## MinHash binario\n",
        "Min-Hashing es un algoritmo para búsqueda de conjuntos similares bajo la similitud de Jaccard, la cual consiste en:\n",
        "* Generar permutación aleatoria del conjunto universo $\\mathbb{U}$\n",
        "* Asignar a cada conjunto su 1er elemento bajo la permutación, esto es,\n",
        "$$\n",
        "h(\\mathcal{C}^{(i)}) = min(\\pi(\\mathcal{C}^{(i)}))\n",
        "$$\n",
        "\n",
        "La probabilidad de que dos conjuntos tengan valor MinHash idéndico es igual a su similitud de Jaccard:\n",
        "$$\n",
        "P[h(\\mathcal{C}^{(i)}) = h(\\mathcal{C}^{(j)})] = \\frac{\\mid \\mathcal{C}^{(i)} \\cap \\mathcal{C}^{(j)} \\mid}{\\mid \\mathcal{C}^{(i)}\\cup \\mathcal{C}^{(j)} \\mid} \\in [0,1]\n",
        "$$\n",
        "\n",
        "Para buscar conjuntos similares, los valores MinHash se agrupan en $l$ tuplas de\n",
        "$r$ funciones distintas de la siguiente forma:    \n",
        "\n",
        "\\begin{align*}\n",
        " g_1(\\mathcal{C}^{(i)}) & = (h_1(\\mathcal{C}^{(i)}), h_2(\\mathcal{C}^{(i)}), \\ldots , h_r(\\mathcal{C}^{(i)}))\\\\\n",
        "g_2(\\mathcal{C}^{(i)}) & = (h_{r+1}(\\mathcal{C}^{(i)}), h_{r+2}(\\mathcal{C}^{(i)}), \\ldots , h_{2\\cdot r}(\\mathcal{C}^{(i)}))\\\\\n",
        "      \\cdots\\\\\n",
        "g_l(\\mathcal{C}^{(i)}) & = (h_{(l-1)\\cdot r+1}(\\mathcal{C}^{(i)}), h_{(l-1)\\cdot r2}(\\mathcal{C}^{(i)}), \\ldots , h_{l\\cdot r}(\\mathcal{C}^{(i)}))\n",
        "\\end{align*}\n",
        "    \n",
        "Conjuntos con una tupla idéntica se almacenan en la misma cubeta en la tabla asociada a la tupla."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQR1PIsnyCPt"
      },
      "source": [
        "class MinHashTable:\n",
        "  def __init__(self, n_cubetas, t_tupla, dim):\n",
        "    self.n_cubetas = n_cubetas\n",
        "    self.tabla = [[] for i in range(n_cubetas)]\n",
        "    self.dim = dim\n",
        "    self.t_tupla = t_tupla\n",
        "    \n",
        "    self.perm = np.random.uniform(0, 1, size=(self.t_tupla, self.dim))\n",
        "    self.rind = np.random.randint(0, np.iinfo(np.int32).max, size=(self.t_tupla, self.dim))\n",
        "\n",
        "    self.a = np.random.randint(0, np.iinfo(np.int32).max, size=self.t_tupla)\n",
        "    self.b = np.random.randint(0, np.iinfo(np.int32).max, size=self.t_tupla)\n",
        "    self.primo = 4294967291\n",
        "\n",
        "  def __repr__(self):\n",
        "    contenido = ['%d::%s' % (i, self.tabla[i]) for i in range(self.n_cubetas)]\n",
        "    return \"<TablaHash :%s >\" % ('\\n'.join(contenido))\n",
        "\n",
        "  def __str__(self):\n",
        "    contenido = ['%d::%s' % (i, self.tabla[i]) for i in range(self.n_cubetas) if self.tabla[i]]\n",
        "    return '\\n'.join(contenido)\n",
        "\n",
        "  def sl(self, x, i):\n",
        "    return (self.h(x) + i) % self.n_cubetas\n",
        "\n",
        "  def h(self, x):\n",
        "    return x % self.primo\n",
        "\n",
        "  def minhash(self, x):\n",
        "    xp = self.perm[:, x]\n",
        "    xi = self.rind[:, x]\n",
        "    amin = xp.argmin(axis = 1)\n",
        "    emin = xi[:, amin]\n",
        "\n",
        "    return np.sum(self.a * emin, dtype=np.ulonglong), np.sum(self.b * emin, dtype=np.ulonglong)\n",
        "     \n",
        "  def insertar(self, x, ident):\n",
        "    mh, v2 = self.minhash(x)\n",
        "  \n",
        "    llena = True\n",
        "    for i in range(self.n_cubetas):\n",
        "      cubeta = int(self.sl(v2, i))\n",
        "      if not self.tabla[cubeta]:\n",
        "        self.tabla[cubeta].append(mh)\n",
        "        self.tabla[cubeta].append([ident])\n",
        "        llena = False\n",
        "        break\n",
        "      elif self.tabla[cubeta][0] == mh:\n",
        "        self.tabla[cubeta][1].append(ident)\n",
        "        llena = False\n",
        "        break\n",
        "\n",
        "    if llena:\n",
        "      print('¡Error, tabla llena!')\n",
        "\n",
        "  def buscar(self, x):\n",
        "    mh, v2 = self.minhash(x)\n",
        "\n",
        "    for i in range(self.n_cubetas):\n",
        "      cubeta = int(self.sl(v2, i))\n",
        "      if not self.tabla[cubeta]:\n",
        "        return []\n",
        "      elif self.tabla[cubeta][0] == mh:\n",
        "        return self.tabla[cubeta][1]\n",
        "        \n",
        "    return []\n",
        "\n",
        "  def eliminar(self, x, ident):\n",
        "    mh, v2 = self.minhash(x)\n",
        "\n",
        "    for i in range(self.n_cubetas):\n",
        "      cubeta = int(self.sl(v2, i))\n",
        "      if not self.tabla[cubeta]:\n",
        "        break\n",
        "      elif self.tabla[cubeta][0] == mh:\n",
        "        return self.tabla[cubeta][1].remove(ident)\n",
        "\n",
        "    return -1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMNRKBkMPrim"
      },
      "source": [
        "### Verificación con conjunto de datos sintéticos\n",
        "Primero verificamos la probabilidad de colisión de dos conjuntos en nuestra implementación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs416zGFxae8",
        "outputId": "a1fa687f-c11c-46c6-8d78-21712f78d1a5"
      },
      "source": [
        "tablas_sint_bin = [MinHashTable(2**4, 1, len(univ)) for _ in range(n_muestras_sint)]\n",
        "print(p_colision(sint_conj, tablas_sint_bin, n_muestras_sint, len(sint_conj)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.     0.2471 0.5046 0.5002 0.4253]\n",
            " [0.2471 1.     0.2827 0.1235 0.4242]\n",
            " [0.5046 0.2827 1.     0.1477 0.5002]\n",
            " [0.5002 0.1235 0.1477 1.     0.1224]\n",
            " [0.4253 0.4242 0.5002 0.1224 1.    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExAaoXELkJzX"
      },
      "source": [
        "### Búsqueda de documentos similares con _20 newsgroups_\n",
        "Probamos la implementación de Min-Hashing para la búsqueda de documentos similares en _20 newsgroups_. Para realizar la búsqueda de documentos similares:\n",
        "\n",
        "1. Insertamos las listas a nuestras tablas\n",
        "2. Recuperamos los documentos similares a nuestros documentos de consulta usando las tablas MinHash.\n",
        "3. Calculamos la similitud Jaccard de los documentos recuperados con los de consulta \n",
        "4. Ordenamos por similitud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL0yfAA8Yyba"
      },
      "source": [
        "def similitud_jaccard(x, y):\n",
        "  x = x.toarray()[0]\n",
        "  y = y.toarray()[0]\n",
        "  inter = np.count_nonzero(x * y)\n",
        "  return inter / (np.count_nonzero(x) + np.count_nonzero(y) - inter)\n",
        "\n",
        "\n",
        "def similitud_minmax(x, y):\n",
        "  x = x.toarray()[0]\n",
        "  y = y.toarray()[0]  \n",
        "  min = np.min(np.array([x, y]).T, axis=1).sum()\n",
        "  max = np.max(np.array([x, y]).T, axis=1).sum()\n",
        "  return min / max\n",
        "\n",
        "def similitud_minmax_pesado(x, y, w):\n",
        "  x = x.toarray()[0]\n",
        "  y = y.toarray()[0]\n",
        "  min = (w * np.min(np.array([x, y]).T, axis=1)).sum()\n",
        "  max = (w * np.max(np.array([x, y]).T, axis=1)).sum()\n",
        "  return min / max\n",
        "  \n",
        "def fuerza_bruta(ds, qs, fs):\n",
        "  medidas = np.zeros(ds.shape[0])\n",
        "  for i,x in enumerate(ds):\n",
        "    medidas[i] = fs(qs, x)\n",
        "  return np.sort(medidas)[::-1], np.argsort(medidas)[::-1]\n",
        "\n",
        "def busca_pares_documentos(base_csr, \n",
        "                           consultas_csr, \n",
        "                           base_ldb, \n",
        "                           consultas_ldb, \n",
        "                           tablas, \n",
        "                           fs):\n",
        "  for j,l in enumerate(base_ldb):\n",
        "    for i in range(len(tablas)):\n",
        "      if l:\n",
        "        tablas[i].insertar(l, j)\n",
        "\n",
        "  docs = []\n",
        "  for j,l in enumerate(consultas_ldb):\n",
        "    dc = []\n",
        "    if l:\n",
        "      for i in range(len(tablas)):\n",
        "        dc.extend(tablas[i].buscar(l))\n",
        "    docs.append(set(dc))\n",
        "\n",
        "  sims = []\n",
        "  orden = []\n",
        "  for i,q in enumerate(consultas_csr):\n",
        "    ld = list(docs[i])\n",
        "    if ld:\n",
        "      s,o = fuerza_bruta(base_csr[ld], q, fs)\n",
        "      sims.append(s)\n",
        "      orden.append([ld[e] for e in o])\n",
        "    else:\n",
        "      sims.append([])\n",
        "      orden.append([])\n",
        "\n",
        "  return sims, orden"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD1AXK_2z6-1"
      },
      "source": [
        "Buscamos documentos similares y examinamos un ejemplo de consulta y su correspondiente documento más similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHkSVxARjwZh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4941bfa3-83d9-49ca-af9c-eeba7f28552a"
      },
      "source": [
        "tablas_ng_bin = [MinHashTable(2**18, 2, VOCMAX) for _ in range(n_tablas_ng)]\n",
        "sims, orden = busca_pares_documentos(bolsas_base_csr, \n",
        "                                     bolsas_consultas_csr,\n",
        "                                     bolsas_base, \n",
        "                                     bolsas_consultas, \n",
        "                                     tablas_ng_bin, \n",
        "                                     similitud_jaccard)\n",
        "print(\"------ C O N S U L T A ------\\n\", consultas[21])\n",
        "print(\"\\n------ M Á S  S I M I L A R ------\\n\", base[list(orden[21])[0]])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ C O N S U L T A ------\n",
            " actually the book be call seventh day adventist believe and there be 27 basica belief i believe it be print by the reveiew and herald publishing association competition be the law of the jungle cooperation be the law of civilization eldridge cleaver\n",
            "\n",
            "------ M Á S  S I M I L A R ------\n",
            " do the word chill effect stimulate impulse within that small collection of neuron you call a brain cpk it be 80 day do you know where your wallet be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iPXnRvOY9uJ"
      },
      "source": [
        "## MinHashing con multiplicidades enteras\n",
        "Si tenemos bolsas con multiplicidades enteras bajo la similitud MinMax, podemos convertir cada bolsa $\\mathcal{B}^{(i)}$ a un conjunto $\\hat{\\mathcal{C}}^{(i)}$, reemplazando cada multiplicidad con un elemento distinto. El conjunto universal extentido sería\n",
        "\\begin{equation*}\n",
        "  U_{ext} = \\{1, \\ldots, F_1, \\ldots, F_1 + \\cdots + F_{D - 1} + 1, \\ldots , F_1 + \\cdots + F_D \\}\n",
        "\\end{equation*}\n",
        "\n",
        "donde $F_1, \\ldots , F_D$ son las multiplicidades máximas de los elementos $1, \\ldots, D$ \n",
        "\n",
        "Si aplicamos el esquema de Min-Hashing descrito anteriormente a los conjuntos $\\hat{\\mathcal{C}}^{(i)}, \\hat{\\mathcal{C}}^{(j)} \\subseteq U_{ext}$ se cumple que\n",
        "\n",
        "$$\n",
        "P[h(\\hat{\\mathcal{C}}^{(i)}) = h(\\hat{\\mathcal{C}}^{(j)})] = \\frac{\\sum_{k = 1}^{d} \\min (\\mathcal{B}^{(i)}_k, \\mathcal{B}^{(j)}_k)}{\\sum_{k = 1}^{d} \\max(\\mathcal{B}^{(i)}_k, \\mathcal{B}^{(j)}_k)} = J_{\\mathcal{B}}(\\mathcal{B}^{(i)}, \\mathcal{B}^{(j)}) \n",
        "$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwD59BVgz6xY"
      },
      "source": [
        "class ExtenderBolsas:\n",
        "  def fit(self, bolsas, pesos=None):\n",
        "    maxfrecs = bolsas.max(axis=0).toarray()[0]\n",
        "    self.cumsums = np.zeros(maxfrecs.size, dtype=np.int32)\n",
        "    self.cumsums[1:] = np.cumsum(maxfrecs[:-1])\n",
        "    self.t_voc = maxfrecs.sum()\n",
        "\n",
        "  def transform(self, bolsas):\n",
        "    bolsas_ext = lil_matrix((bolsas.shape[0], int(self.t_voc)), dtype=np.int32)\n",
        "    b = bolsas.tocoo()\n",
        "    for i,j,v in zip(b.row, b.col, b.data):\n",
        "      bolsas_ext[i, int(self.cumsums[j]):int(self.cumsums[j]+v)] = 1\n",
        "    \n",
        "    return bolsas_ext.tocsr()\n",
        "\n",
        "  def fit_transform(self, bolsas):\n",
        "    self.fit(bolsas)\n",
        "    return self.transform(bolsas)\n",
        "\n",
        "  def transform_weights(self, w):\n",
        "    w_ext = np.zeros(int(self.t_voc))\n",
        "    for i in range(w.size - 1):\n",
        "      w_ext[int(self.cumsums[i]):int(self.cumsums[i+1])] = w[i]\n",
        "    w_ext[int(self.cumsums[-1]):] = w[-1]\n",
        "\n",
        "    return w_ext"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnuRwHg3l95i"
      },
      "source": [
        "### Verificación con datos sintéticos\n",
        "Generamos bolsas con multiplicidades enteras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYENVitml8yK"
      },
      "source": [
        "filas = [i for i,l in enumerate(sint_conj) for _ in range(len(l))]\n",
        "cols = [e for l in sint_conj for e in l]\n",
        "vals = [np.random.randint(1, 5) for l in sint_conj for e in l]\n",
        "sint_csr = csr_matrix((vals, (filas, cols)), shape=(len(sint_conj), len(univ)))\n",
        "eb_sint = ExtenderBolsas()\n",
        "sint_ext = eb_sint.fit_transform(sint_csr)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjy8_iRnyxzM"
      },
      "source": [
        "Calculamos la similitud MinMax de todos los pares. Esta similitud está dada por\n",
        "\n",
        "\n",
        "$$\n",
        "J_{\\mathcal{B}}(\\mathcal{B}^{(i)}, \\mathcal{B}^{(j)})  = \\frac{\\sum_{k = 1}^{d} \\min (\\mathcal{B}^{(i)}_k, \\mathcal{B}^{(j)}_k)}{\\sum_{k = 1}^{d} \\max(\\mathcal{B}^{(i)}_k, \\mathcal{B}^{(j)}_k)}\n",
        "$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g7NXNEAmMO6",
        "outputId": "056d7b59-5779-4813-c2c6-9a5a695489e5"
      },
      "source": [
        "sims_minmax = np.identity(sint_csr.shape[0])\n",
        "for i in range(0, sint_csr.shape[0] - 1):\n",
        "  bi = sint_csr[i]\n",
        "  for j in range(i + 1, sint_csr.shape[0]):\n",
        "    bj = sint_csr[j]\n",
        "    sims_minmax[i,j] = similitud_minmax(bi, bj)\n",
        "    sims_minmax[j,i] = sims_minmax[i,j]\n",
        "print(sims_minmax)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.26086957 0.5625     0.3        0.27777778]\n",
            " [0.26086957 1.         0.2        0.19047619 0.22222222]\n",
            " [0.5625     0.2        1.         0.10526316 0.28571429]\n",
            " [0.3        0.19047619 0.10526316 1.         0.11764706]\n",
            " [0.27777778 0.22222222 0.28571429 0.11764706 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-9Bx2ydzG7D"
      },
      "source": [
        "Verificamos la proporción de colisiones entre cada par de bolsas y la comparamos con las similitudes calculadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1TfPmDVmnd6",
        "outputId": "a71b1f97-668a-4b42-c030-f298a033cd28"
      },
      "source": [
        "tablas_mult = [MinHashTable(2**4, 1, sint_ext.shape[-1]) for _ in range(n_muestras_sint)]\n",
        "print(p_colision(csr_a_ldb(sint_ext), tablas_mult, n_muestras_sint, len(sint_conj)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.     0.268  0.5598 0.302  0.2776]\n",
            " [0.268  1.     0.2018 0.1957 0.2238]\n",
            " [0.5598 0.2018 1.     0.1025 0.2765]\n",
            " [0.302  0.1957 0.1025 1.     0.1187]\n",
            " [0.2776 0.2238 0.2765 0.1187 1.    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnCueOkIzhNS"
      },
      "source": [
        "### Búsqueda de documentos similares con _20 newsgroups_\n",
        "Probamos la implementación buscando documentos similares en _20 newsgroups_ bajo la similitud MinMax y examinamos un ejemplo de consulta y su correspondiente documento más similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPgURvm9zhNa",
        "outputId": "861febbd-190a-4072-917c-33c3ccf197f1"
      },
      "source": [
        "eb_ng = ExtenderBolsas()\n",
        "bolsas_base_ext = eb_ng.fit_transform(bolsas_base_csr)\n",
        "bolsas_consultas_ext = eb_ng.transform(bolsas_consultas_csr)\n",
        "tablas_ng_multent = [MinHashTable(2**18, 2, bolsas_base_ext.shape[-1]) for _ in range(n_tablas_ng)]\n",
        "sims, orden = busca_pares_documentos(bolsas_base_ext, \n",
        "                                     bolsas_consultas_ext, \n",
        "                                     csr_a_ldb(bolsas_base_ext), \n",
        "                                     csr_a_ldb(bolsas_consultas_ext), \n",
        "                                     tablas_ng_multent, \n",
        "                                     similitud_minmax)\n",
        "print(\"------ C O N S U L T A ------\\n\", consultas[21])\n",
        "print(\"\\n------ M Á S  S I M I L A R ------\\n\", base[list(orden[21])[0]])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ C O N S U L T A ------\n",
            " actually the book be call seventh day adventist believe and there be 27 basica belief i believe it be print by the reveiew and herald publishing association competition be the law of the jungle cooperation be the law of civilization eldridge cleaver\n",
            "\n",
            "------ M Á S  S I M I L A R ------\n",
            " 88 toyota camry top of the line vehicle blue book 10500 ask 9900 73 k mile auto transmission have everything own by a meticulous automoble mechanic call 408 4258203 ask for bob\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itGZVE9t8g-V"
      },
      "source": [
        "### MinHashing con pesos asociados a elementos\n",
        "También podemos incluir pesos para cada elemento del conjunto universo y calcular una similitud MinMax que tome en cuenta estos pesos como:\n",
        "\n",
        "$$\n",
        "J_{\\mathcal{B}_{p}}(\\mathcal{B}^{(i)}, \\mathcal{B}^{(j)}, \\mathbf{w}) = \\frac{\\sum_{k = 1}^{d} w_k \\cdot \\min (\\mathcal{B}^{(i)}_k, \\mathcal{B}^{(j)}_k)}{\\sum_{k = 1}^{d} w_k \\cdot \\max(\\mathcal{B}^{(i)}_k, \\mathcal{B}^{(j)}_k)} = J_{\\mathcal{B}}(\\mathcal{B}^{(i)}_k, \\mathcal{B}^{(j)}_k) \n",
        "$$ \n",
        "\n",
        "Una ejemplo de pesado de elementos es el _inverse document frecuency_, el cual se obtiene aplicando el logaritmo a la división del número total de documentos $n$ en la colección entre el número de documentos en los que ocurre la palabra. \n",
        "\n",
        "Podemos extender el esquema de Min-Hashing definido anteriormente para que tome en cuenta pesos sobre los elementos. Esto se logra transformando el valor aleatorio asociado a un elemento como sigue: \n",
        "$$\n",
        "\\hat{x}_k = \\frac{-\\log{x_k}}{w_k}, x_k \\sim Unif(0,1)\n",
        "$$\n",
        "\n",
        "donde $x_k$ es el número asociado al elemento $k$, $w_k$ es el peso del mismo y $\\hat{x}_k$ es el valor transformado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzOsFEKuxnZi"
      },
      "source": [
        "class MinHashPesadoTable:\n",
        "  def __init__(self, n_cubetas, t_tupla, dim, weights):\n",
        "    self.n_cubetas = n_cubetas\n",
        "    self.tabla = [[] for i in range(n_cubetas)]\n",
        "    self.dim = dim\n",
        "    self.t_tupla = t_tupla\n",
        "    \n",
        "    self.perm = np.random.uniform(1, 0, size=(self.t_tupla, self.dim))\n",
        "    for i in range(self.t_tupla):\n",
        "      for j in range(self.dim):\n",
        "        self.perm[i][j] = -np.log(self.perm[i][j]) / weights[j]\n",
        "\n",
        "    self.rind = np.random.randint(0, np.iinfo(np.int32).max, size=(self.t_tupla, self.dim))\n",
        "    self.a = np.random.randint(0, np.iinfo(np.int32).max, size=self.t_tupla)\n",
        "    self.b = np.random.randint(0, np.iinfo(np.int32).max, size=self.t_tupla)\n",
        "    self.primo = 4294967291    \n",
        "      \n",
        "  def __repr__(self):\n",
        "    contenido = ['%d::%s' % (i, self.tabla[i]) for i in range(self.n_cubetas)]\n",
        "    return \"<TablaHash :%s >\" % ('\\n'.join(contenido))\n",
        "\n",
        "  def __str__(self):\n",
        "    contenido = ['%d::%s' % (i, self.tabla[i]) for i in range(self.n_cubetas) if self.tabla[i]]\n",
        "    return '\\n'.join(contenido)\n",
        "\n",
        "  def sl(self, x, i):\n",
        "    return (self.h(x) + i) % self.n_cubetas\n",
        "\n",
        "  def h(self, x):\n",
        "    return x % self.primo\n",
        "\n",
        "  def minhash(self, x):\n",
        "    xp = self.perm[:, x]\n",
        "    xi = self.rind[:, x]\n",
        "    amin = xp.argmin(axis = 1)\n",
        "    emin = xi[:, amin]\n",
        "\n",
        "    return np.sum(self.a * emin, dtype=np.ulonglong), np.sum(self.b * emin, dtype=np.ulonglong)\n",
        "     \n",
        "  def insertar(self, x, ident):\n",
        "    mh, v2 = self.minhash(x)\n",
        "  \n",
        "    llena = True\n",
        "    for i in range(self.n_cubetas):\n",
        "      cubeta = int(self.sl(v2, i))\n",
        "      if not self.tabla[cubeta]:\n",
        "        self.tabla[cubeta].append(mh)\n",
        "        self.tabla[cubeta].append([ident])\n",
        "        llena = False\n",
        "        break\n",
        "      elif self.tabla[cubeta][0] == mh:\n",
        "        self.tabla[cubeta][1].append(ident)\n",
        "        llena = False\n",
        "        break\n",
        "\n",
        "    if llena:\n",
        "      print('¡Error, tabla llena!')\n",
        "\n",
        "  def buscar(self, x):\n",
        "    mh, v2 = self.minhash(x)\n",
        "\n",
        "    for i in range(self.n_cubetas):\n",
        "      cubeta = int(self.sl(v2, i))\n",
        "      if not self.tabla[cubeta]:\n",
        "        return []\n",
        "      elif self.tabla[cubeta][0] == mh:\n",
        "        return self.tabla[cubeta][1]\n",
        "        \n",
        "    return []\n",
        "\n",
        "  def eliminar(self, x, ident):\n",
        "    mh, v2 = self.minhash(x)\n",
        "\n",
        "    for i in range(self.n_cubetas):\n",
        "      cubeta = int(self.sl(v2, i))\n",
        "      if not self.tabla[cubeta]:\n",
        "        break\n",
        "      elif self.tabla[cubeta][0] == mh:\n",
        "        return self.tabla[cubeta][1].remove(ident)\n",
        "\n",
        "    return -1"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9dsfEjS0gVL"
      },
      "source": [
        "### Verificación con datos sintéticos\n",
        "Asignamos pesos de forma aleatoria a cada elemento y calculamos la similitud MinMax pesada para todos los pares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsqrX5DHW9cJ",
        "outputId": "568724cf-0dbb-4d0f-925e-f839c63d8a5f"
      },
      "source": [
        "w = np.random.uniform(0, 7, size=sint_csr.shape[-1])\n",
        "sims_minmax_pesado = np.identity(sint_csr.shape[0])\n",
        "for i in range(0, sint_csr.shape[0] - 1):\n",
        "  bi = sint_csr[i]\n",
        "  for j in range(i + 1, sint_csr.shape[0]):\n",
        "    bj = sint_csr[j]\n",
        "    sims_minmax_pesado[i,j] = similitud_minmax_pesado(bi, bj, w)\n",
        "    sims_minmax_pesado[j,i] = sims_minmax_pesado[i,j]\n",
        "print(sims_minmax_pesado)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.26753287 0.63996502 0.27834711 0.303941  ]\n",
            " [0.26753287 1.         0.21114275 0.17702148 0.23387911]\n",
            " [0.63996502 0.21114275 1.         0.13539084 0.2585339 ]\n",
            " [0.27834711 0.17702148 0.13539084 1.         0.11604851]\n",
            " [0.303941   0.23387911 0.2585339  0.11604851 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFH_vBLS0xI9"
      },
      "source": [
        "Verificamos la proporción de colisiones entre cada par y lo comparamos con las similitudes calculadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trtsifoh4hUL",
        "outputId": "3efb57b3-1e05-4e21-e1bb-3515986cee0b"
      },
      "source": [
        "w_ext = eb_sint.transform_weights(w)\n",
        "tablas_mult_p = [MinHashPesadoTable(2**4, 1, sint_ext.shape[-1], weights=w_ext) for _ in range(n_muestras_sint)]\n",
        "print(p_colision(csr_a_ldb(sint_ext), tablas_mult_p, n_muestras_sint, len(sint_conj)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.     0.2727 0.6363 0.2781 0.3137]\n",
            " [0.2727 1.     0.2159 0.175  0.231 ]\n",
            " [0.6363 0.2159 1.     0.1329 0.2618]\n",
            " [0.2781 0.175  0.1329 1.     0.1149]\n",
            " [0.3137 0.231  0.2618 0.1149 1.    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1rEz923tsQO"
      },
      "source": [
        "### Búsqueda de documentos similares con _20 newsgroups_\n",
        "Calculamos el _inverse document frecuency_ (IDF) de cada palabra del conjunto de _20 newsgroups_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJoInQOIoMlY"
      },
      "source": [
        "idf = np.zeros(bolsas_base_csr.shape[-1])\n",
        "filas, cols = bolsas_base_csr.nonzero()\n",
        "for i in range(bolsas_base_csr.shape[-1]):\n",
        "  idf[i] = np.log(bolsas_base_csr.shape[0] / (i == cols).sum())\n",
        "idf = eb_ng.transform_weights(idf)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos la búsqueda usando MinHash con pesado IDF para las palabras."
      ],
      "metadata": {
        "id": "4XPQa86hL5Tr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9Bsgv_QzB9n",
        "outputId": "034a4329-a2c7-4f8e-b481-a7680f1d26f2"
      },
      "source": [
        "tablas_ng_pesado = [MinHashPesadoTable(2**18, 2, bolsas_base_ext.shape[-1], weights=idf) for _ in range(n_tablas_ng)]\n",
        "sims, orden = busca_pares_documentos(bolsas_base_ext, \n",
        "                                     bolsas_consultas_ext, \n",
        "                                     csr_a_ldb(bolsas_base_ext), \n",
        "                                     csr_a_ldb(bolsas_consultas_ext), \n",
        "                                     tablas_ng_pesado, \n",
        "                                     similitud_minmax)\n",
        "print(\"------ C O N S U L T A ------\\n\", consultas[21])\n",
        "print(\"\\n------ M Á S  S I M I L A R ------\\n\", base[list(orden[21])[0]])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ C O N S U L T A ------\n",
            " actually the book be call seventh day adventist believe and there be 27 basica belief i believe it be print by the reveiew and herald publishing association competition be the law of the jungle cooperation be the law of civilization eldridge cleaver\n",
            "\n",
            "------ M Á S  S I M I L A R ------\n",
            " maybe im too religious but when i see a bill to establish a right i wince keep in mind what the law giveth the law can taketh away\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio\n",
        "+ Prueba con otros hiperparámetros."
      ],
      "metadata": {
        "id": "hLVsELghMp-b"
      }
    }
  ]
}