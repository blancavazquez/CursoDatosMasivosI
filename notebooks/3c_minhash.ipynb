{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3c_minhash.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IvxCNshmnMDh"
      ],
      "authorship_tag": "ABX9TyP1xzKhnyh7d+2XltQ0t6rr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blancavazquez/CursoDatosMasivosI/blob/master/notebooks/3c_minhash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kloh2BluGySD"
      },
      "source": [
        "# Búsqueda de documentos con MinHash\n",
        "En esta libreta veremos cómo hacer búsqueda eficiente de documentos considerando la similitud de Jaccard.\n",
        "\n",
        "La similitud de Jaccard entre un par de conjuntos $(\\mathcal{C}^{(1)}, \\mathcal{C}^{(2)})$ está dada por\n",
        "\n",
        "$$\n",
        "J(\\mathcal{C}^{(1)}, \\mathcal{C}^{(2)}) = \\frac{\\mid \\mathcal{C}^{(1)} \\cap \\mathcal{C}^{(2)} \\mid}{\\mid \\mathcal{C}^{(1)}\\cup \\mathcal{C}^{(2)} \\mid} \\in [0,1]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOzaNGg6PA6_"
      },
      "source": [
        "from collections import Counter\n",
        "from math import floor, log\n",
        "import codecs \n",
        "import re \n",
        "\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix, lil_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "VOCMAX = 5000\n",
        "n_muestras_sint = 10000\n",
        "n_tablas_ng = 50\n",
        "\n",
        "# Para reproducibilidad\n",
        "np.random.seed(2021)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOhZkfFSzAQE"
      },
      "source": [
        "## Conjuntos de datos\n",
        "Preparamos dos conjuntos de datos para probar los algoritmos de búsqueda.\n",
        "\n",
        "### Datos sintéticos\n",
        "Este conjunto de datos está compuesto por 5 listas de elementos y un conjunto universo de 10 elementos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te6Uqrb6zFwS"
      },
      "source": [
        "sint_conj = [[0, 1, 4, 6, 8], [2, 3, 4, 7, 8], [1, 4, 6, 7], [0, 5, 6, 8], [0, 1, 3, 4, 7]]\n",
        "univ = {e for l in sint_conj for e in l}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AUc2a-ure1E"
      },
      "source": [
        "Calculamos la similitud de Jaccard de todos los pares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oab1l7qyfkUV",
        "outputId": "b7a20d71-a570-4229-bb10-396b62e5e709"
      },
      "source": [
        "sims_jacc = np.identity(len(sint_conj))\n",
        "for i in range(0, len(sint_conj) - 1):\n",
        "  ci = set(sint_conj[i])\n",
        "  for j in range(i + 1, len(sint_conj)):\n",
        "    cj = set(sint_conj[j])\n",
        "    sims_jacc[i,j] = float(len(ci.intersection(cj)) / len(ci.union(cj)))\n",
        "    sims_jacc[j,i] = sims_jacc[i,j]\n",
        "\n",
        "print(sims_jacc)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.25       0.5        0.5        0.42857143]\n",
            " [0.25       1.         0.28571429 0.125      0.42857143]\n",
            " [0.5        0.28571429 1.         0.14285714 0.5       ]\n",
            " [0.5        0.125      0.14285714 1.         0.125     ]\n",
            " [0.42857143 0.42857143 0.5        0.125      1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lfZa7s6sE8U"
      },
      "source": [
        "Definimos una función que calcula la propiedad de colisión de todos los pares en este conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIYV78vTsaTk"
      },
      "source": [
        "def p_colision(db, tablas, n_tablas, n_ej):\n",
        "  for i in range(n_tablas):\n",
        "    for j,l in enumerate(db):\n",
        "      tablas[i].insertar(l, j)\n",
        "\n",
        "  colisiones = np.zeros((n_ej, n_ej))\n",
        "  for i in range(n_tablas):\n",
        "    for j,cj in enumerate(db):\n",
        "      for e in tablas[i].buscar(cj):\n",
        "        colisiones[j, e] += 1\n",
        "\n",
        "  return colisiones / n_tablas"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_56-act4ikel"
      },
      "source": [
        "### 20 Newsgroups\n",
        "Vamos a usar el conjunto de documentos de _20 Newsgropus_, el cual descargamos usando scikit-learn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrIWmtSvLthy"
      },
      "source": [
        "db = fetch_20newsgroups(remove=('headers','footers','quotes'))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drmTbXrFLdoL"
      },
      "source": [
        "Importamos la biblioteca NLTK y definimos nuestro analizador léxico y lematizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9HE8AS41yYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99df758d-d502-4fd4-a57c-744cfef54594"
      },
      "source": [
        "import nltk\n",
        "nltk.download(['punkt','averaged_perceptron_tagger','wordnet'])\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus.reader.wordnet import NOUN, VERB, ADV, ADJ\n",
        "\n",
        "morphy_tag = {\n",
        "    'JJ' : ADJ,\n",
        "    'JJR' : ADJ,\n",
        "    'JJS' : ADJ,\n",
        "    'VB' : VERB,\n",
        "    'VBD' : VERB,\n",
        "    'VBG' : VERB,\n",
        "    'VBN' : VERB,\n",
        "    'VBP' : VERB,\n",
        "    'VBZ' : VERB,\n",
        "    'RB' : ADV,\n",
        "    'RBR' : ADV,\n",
        "    'RBS' : ADV\n",
        "}\n",
        "\n",
        "def doc_a_tokens(doc):\n",
        "  tagged = pos_tag(word_tokenize(doc.lower()))\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = []\n",
        "  for p,t in tagged:\n",
        "    tokens.append(lemmatizer.lemmatize(p, pos=morphy_tag.get(t, NOUN)))\n",
        "\n",
        "  return tokens"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBHJ4RGvLzZB"
      },
      "source": [
        "Convertimos el conjunto preprocesado a una lista de cadenas, una por documento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv7kFB151Kpi"
      },
      "source": [
        "corpus = []\n",
        "for d in db.data:\n",
        "  d = d.replace('\\n',' ').replace('\\r',' ').replace('\\t',' ')\n",
        "  d = ' '.join([''.join([c.lower() for c in p if c.isalnum()]) for p in d.split()])\n",
        "  tokens = doc_a_tokens(d)\n",
        "  corpus.append(' '.join(tokens))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ado-nD45jFNP"
      },
      "source": [
        "Dividimos nuestro conjunto en 2 subconjuntos: los documentos de la base que se buscarán y los documentos de consulta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cduiS-ukgm7m"
      },
      "source": [
        "perm = np.random.permutation(len(corpus)).astype(int)\n",
        "n_ej_base = int(floor(len(corpus) * 0.95))\n",
        "\n",
        "base = [corpus[i] for i in perm[:n_ej_base]]\n",
        "consultas = [corpus[i] for i in perm[n_ej_base:]]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos y cargamos la lista de _stopwords_ para inglés (archivo con una palabra por línea)"
      ],
      "metadata": {
        "id": "ZbvR3-DPyElj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -qO- -O stopwords_english.txt \\\n",
        "         https://raw.githubusercontent.com/pan-webis-de/authorid/master/data/stopwords_english.txt\n",
        "\n",
        "stopwords = []\n",
        "for line in codecs.open('stopwords_english.txt', encoding = \"utf-8\"):\n",
        "  stopwords.append(line.rstrip())"
      ],
      "metadata": {
        "id": "EywC1QNIc9If"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procesamos cada palabra del corpus completo para generar y ordenar el vocabulario"
      ],
      "metadata": {
        "id": "Z46cJRKQyk2r"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3xcTfR7y8wE"
      },
      "source": [
        "# Divide la cadena en palabras\n",
        "term_re = re.compile(\"\\w+\", re.UNICODE)\n",
        "\n",
        "# Contamos las ocurrencias de cada palabra\n",
        "corpus_freq = Counter()\n",
        "doc_freq = Counter()\n",
        "for d in base:\n",
        "  # Eliminamos números de la cadena (documento) a procesar \n",
        "  d = re.sub(r'\\d+', '', d)\n",
        "\n",
        "  # Dividimos la cadena en una lista de palabras\n",
        "  terms = [t for t in term_re.findall(d) if t not in stopwords and len(t) > 2]\n",
        "  \n",
        "  # Aumentamos el contador de cada instancia palabra en el documento\n",
        "  for t in terms:\n",
        "    corpus_freq[t] += 1\n",
        "  \n",
        "  # Aumentamos el contador de cada palabra distinta en el documento\n",
        "  for t in set(terms):\n",
        "    doc_freq[t] += 1\n",
        "\n",
        "# Generamos un diccionario con las VOCMAX palabras más frecuentes\n",
        "vocabulary = {entry[0]:(i, entry[1], doc_freq[entry[0]], log(len(corpus) / doc_freq[entry[0]])) \\\n",
        "              for i, entry in enumerate(corpus_freq.most_common()) \\\n",
        "              if i < VOCMAX}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos un diccionario para mapear índices a palabras"
      ],
      "metadata": {
        "id": "K4yw8fgsA5SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id_a_palabra = {v[0]: k for k,v in vocabulary.items()}"
      ],
      "metadata": {
        "id": "riRxAcyuBFAy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3JFW9TJL-ne"
      },
      "source": [
        "Generamos las bolsas de palabras de los documentos preprocesados"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cadenas_a_bolsas(cadenas, voc, descartar, tre):\n",
        "  bolsas = []\n",
        "  for c in cadenas:\n",
        "    c = re.sub(r'\\d+', '', c)\n",
        "    ids = Counter([voc[t][0] for t in tre.findall(c) \\\n",
        "                   if t in voc and t not in descartar])\n",
        "    bolsas.append([i for i in sorted(ids.items())])\n",
        "\n",
        "  return bolsas\n",
        "\n",
        "bolsas_base = cadenas_a_bolsas(base, vocabulary, stopwords, term_re)\n",
        "bolsas_consultas = cadenas_a_bolsas(consultas, vocabulary, stopwords, term_re)"
      ],
      "metadata": {
        "id": "xIzDYE8beEhP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgDZWHzhru4b"
      },
      "source": [
        "## MinHash binario\n",
        "Min-Hashing es un algoritmo para búsqueda de conjuntos similares bajo la similitud de Jaccard, la cual consiste en:\n",
        "* Generar permutación aleatoria del conjunto universo $\\mathbb{U}$\n",
        "* Asignar a cada conjunto su 1er elemento bajo la permutación, esto es,\n",
        "$$\n",
        "h(\\mathcal{C}^{(i)}) = min(\\pi(\\mathcal{C}^{(i)}))\n",
        "$$\n",
        "\n",
        "La probabilidad de que dos conjuntos tengan valor MinHash idéndico es igual a su similitud de Jaccard:\n",
        "$$\n",
        "P[h(\\mathcal{C}^{(i)}) = h(\\mathcal{C}^{(j)})] = \\frac{\\mid \\mathcal{C}^{(i)} \\cap \\mathcal{C}^{(j)} \\mid}{\\mid \\mathcal{C}^{(i)}\\cup \\mathcal{C}^{(j)} \\mid} \\in [0,1]\n",
        "$$\n",
        "\n",
        "Para buscar conjuntos similares, los valores MinHash se agrupan en $l$ tuplas de\n",
        "$r$ funciones distintas de la siguiente forma:    \n",
        "\n",
        "\\begin{align*}\n",
        " g_1(\\mathcal{C}^{(i)}) & = (h_1(\\mathcal{C}^{(i)}), h_2(\\mathcal{C}^{(i)}), \\ldots , h_r(\\mathcal{C}^{(i)}))\\\\\n",
        "g_2(\\mathcal{C}^{(i)}) & = (h_{r+1}(\\mathcal{C}^{(i)}), h_{r+2}(\\mathcal{C}^{(i)}), \\ldots , h_{2\\cdot r}(\\mathcal{C}^{(i)}))\\\\\n",
        "      \\cdots\\\\\n",
        "g_l(\\mathcal{C}^{(i)}) & = (h_{(l-1)\\cdot r+1}(\\mathcal{C}^{(i)}), h_{(l-1)\\cdot r2}(\\mathcal{C}^{(i)}), \\ldots , h_{l\\cdot r}(\\mathcal{C}^{(i)}))\n",
        "\\end{align*}\n",
        "    \n",
        "Conjuntos con una tupla idéntica se almacenan en la misma cubeta en la tabla asociada a la tupla."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQR1PIsnyCPt"
      },
      "source": [
        "class MinHashTable:\n",
        "  def __init__(self, n_cubetas, t_tupla, dim):\n",
        "    self.n_cubetas = n_cubetas\n",
        "    self.tabla = [[] for i in range(n_cubetas)]\n",
        "    self.dim = dim\n",
        "    self.t_tupla = t_tupla\n",
        "    \n",
        "    self.perm = np.random.uniform(0, 1, size=(self.t_tupla, self.dim))\n",
        "    self.rind = np.random.randint(0, np.iinfo(np.int32).max, size=(self.t_tupla, self.dim))\n",
        "\n",
        "    self.a = np.random.randint(0, np.iinfo(np.int32).max, size=self.t_tupla)\n",
        "    self.b = np.random.randint(0, np.iinfo(np.int32).max, size=self.t_tupla)\n",
        "    self.primo = 4294967291\n",
        "\n",
        "  def __repr__(self):\n",
        "    contenido = ['%d::%s' % (i, self.tabla[i]) for i in range(self.n_cubetas)]\n",
        "    return \"<TablaHash :%s >\" % ('\\n'.join(contenido))\n",
        "\n",
        "  def __str__(self):\n",
        "    contenido = ['%d::%s' % (i, self.tabla[i]) for i in range(self.n_cubetas) if self.tabla[i]]\n",
        "    return '\\n'.join(contenido)\n",
        "\n",
        "  def sl(self, x, i):\n",
        "    return (self.h(x) + i) % self.n_cubetas\n",
        "\n",
        "  def h(self, x):\n",
        "    return x % self.primo\n",
        "\n",
        "  def minhash(self, x):\n",
        "    xp = self.perm[:, x]\n",
        "    xi = self.rind[:, x]\n",
        "    amin = xp.argmin(axis = 1)\n",
        "    emin = xi[:, amin]\n",
        "\n",
        "    return np.sum(self.a * emin, dtype=np.ulonglong), np.sum(self.b * emin, dtype=np.ulonglong)\n",
        "     \n",
        "  def insertar(self, x, ident):\n",
        "    mh, v2 = self.minhash(x)\n",
        "  \n",
        "    llena = True\n",
        "    for i in range(self.n_cubetas):\n",
        "      cubeta = int(self.sl(v2, i))\n",
        "      if not self.tabla[cubeta]:\n",
        "        self.tabla[cubeta].append(mh)\n",
        "        self.tabla[cubeta].append([ident])\n",
        "        llena = False\n",
        "        break\n",
        "      elif self.tabla[cubeta][0] == mh:\n",
        "        self.tabla[cubeta][1].append(ident)\n",
        "        llena = False\n",
        "        break\n",
        "\n",
        "    if llena:\n",
        "      print('¡Error, tabla llena!')\n",
        "\n",
        "  def buscar(self, x):\n",
        "    mh, v2 = self.minhash(x)\n",
        "\n",
        "    for i in range(self.n_cubetas):\n",
        "      cubeta = int(self.sl(v2, i))\n",
        "      if not self.tabla[cubeta]:\n",
        "        return []\n",
        "      elif self.tabla[cubeta][0] == mh:\n",
        "        return self.tabla[cubeta][1]\n",
        "        \n",
        "    return []\n",
        "\n",
        "  def eliminar(self, x, ident):\n",
        "    mh, v2 = self.minhash(x)\n",
        "\n",
        "    for i in range(self.n_cubetas):\n",
        "      cubeta = int(self.sl(v2, i))\n",
        "      if not self.tabla[cubeta]:\n",
        "        break\n",
        "      elif self.tabla[cubeta][0] == mh:\n",
        "        return self.tabla[cubeta][1].remove(ident)\n",
        "\n",
        "    return -1"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMNRKBkMPrim"
      },
      "source": [
        "### Verificación con conjunto de datos sintéticos\n",
        "Primero verificamos la probabilidad de colisión de dos conjuntos en nuestra implementación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs416zGFxae8",
        "outputId": "55a5eae0-7326-49ba-d8d0-218ebf311db6"
      },
      "source": [
        "tablas_sint_bin = [MinHashTable(2**4, 1, len(univ)) for _ in range(n_muestras_sint)]\n",
        "print(p_colision(sint_conj, tablas_sint_bin, n_muestras_sint, len(sint_conj)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.     0.2471 0.5046 0.5002 0.4253]\n",
            " [0.2471 1.     0.2827 0.1235 0.4242]\n",
            " [0.5046 0.2827 1.     0.1477 0.5002]\n",
            " [0.5002 0.1235 0.1477 1.     0.1224]\n",
            " [0.4253 0.4242 0.5002 0.1224 1.    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExAaoXELkJzX"
      },
      "source": [
        "### Búsqueda de documentos similares con _20 newsgroups_\n",
        "Probamos la implementación de Min-Hashing para la búsqueda de documentos similares en _20 newsgroups_. Para realizar la búsqueda de documentos similares:\n",
        "\n",
        "1. Insertamos las listas a nuestras tablas\n",
        "2. Recuperamos los documentos similares a nuestros documentos de consulta usando las tablas MinHash.\n",
        "3. Calculamos la similitud Jaccard de los documentos recuperados con los de consulta \n",
        "4. Ordenamos por similitud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL0yfAA8Yyba"
      },
      "source": [
        "def similitud_jaccard(x, y):\n",
        "  xa = np.zeros(VOCMAX)\n",
        "  xa[x] = 1\n",
        "  ya = np.zeros(VOCMAX)\n",
        "  xa[x] = 1\n",
        "  \n",
        "  inter = np.count_nonzero(x * y)\n",
        "  return inter / (np.count_nonzero(x) + np.count_nonzero(y) - inter)\n",
        "\n",
        "def fuerza_bruta(ds, qs, fs):\n",
        "  medidas = np.zeros(ds.shape[0])\n",
        "  for i,x in enumerate(ds):\n",
        "    medidas[i] = fs(qs, x)\n",
        "  return np.sort(medidas)[::-1], np.argsort(medidas)[::-1]\n",
        "\n",
        "def busca_pares_documentos(ll_base, ll_consulta, tablas, fs):\n",
        "  for j,l in enumerate(ll_base):\n",
        "    for i in range(len(tablas)):\n",
        "      tablas[i].insertar(l, j)\n",
        "\n",
        "  docs = []\n",
        "  for j,l in enumerate(ll_consultas):\n",
        "    dc = []\n",
        "    for i in range(len(tablas)):\n",
        "      dc.extend(tablas[i].buscar(l))\n",
        "    docs.append(set(dc))\n",
        "\n",
        "  sims = []\n",
        "  orden = []\n",
        "  for i,q in enumerate(docs_consulta):\n",
        "    ld = list(docs[i])\n",
        "    if ld:\n",
        "      s,o = fuerza_bruta(docs_base[ld], q, fs)\n",
        "      sims.append(s)\n",
        "      orden.append([ld[e] for e in o])\n",
        "    else:\n",
        "      sims.append([])\n",
        "      orden.append([])\n",
        "\n",
        "  return sims, orden"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD1AXK_2z6-1"
      },
      "source": [
        "Buscamos documentos similares y examinamos un ejemplo de consulta y su correspondiente documento más similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHkSVxARjwZh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "92fad930-3882-4773-e544-7de65558ba11"
      },
      "source": [
        "tablas_ng_bin = [MinHashTable(2**18, 2, VOCMAX) for _ in range(n_tablas_ng)]\n",
        "sims, orden = busca_pares_documentos(bolsas_base, bolsas_consultas, tablas_ng_bin, similitud_jaccard)\n",
        "print(\"------ C O N S U L T A ------\\n\", consultas[1])\n",
        "print(\"\\n------ M Á S  S I M I L A R ------\\n\", base[list(orden[1])[0]])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-338ba807e730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtablas_ng_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mMinHashTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCMAX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tablas_ng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbusca_pares_documentos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbolsas_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbolsas_consultas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtablas_ng_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilitud_jaccard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------ C O N S U L T A ------\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsultas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n------ M Á S  S I M I L A R ------\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-0ed54e7b0947>\u001b[0m in \u001b[0;36mbusca_pares_documentos\u001b[0;34m(ll_base, ll_consulta, tablas, fs)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mll_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtablas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0mtablas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsertar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-d45c783428a4>\u001b[0m in \u001b[0;36minsertar\u001b[0;34m(self, x, ident)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minsertar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mmh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mllena\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-d45c783428a4>\u001b[0m in \u001b[0;36mminhash\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mxp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mamin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0memin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: attempt to get argmin of an empty sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvxCNshmnMDh"
      },
      "source": [
        "## Ejercicio\n",
        "+ Evalúa la búsqueda con distintos valores de $r$ y $\\eta$ usando la fórmula de \n",
        "$$\n",
        "  l = \\frac{log(0.5)}{log(1 - \\eta^r)}\n",
        "$$\n",
        "\n",
        "+ Extiende la clase `MinHashTable` para que tome en cuenta las multiplicidades de las bolsas."
      ]
    }
  ]
}